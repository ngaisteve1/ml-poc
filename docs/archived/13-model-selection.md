# How to Determine the Best Model

## Quick Decision Matrix

| Metric | What It Means | Target | Priority |
|--------|---------------|--------|----------|
| **Test RÂ²** | How well model fits test data | > 0.75 | ğŸ”´ HIGH |
| **Test MAE** | Average prediction error (GB) | < 10 | ğŸ”´ HIGH |
| **Test RMSE** | Error considering large mistakes | < 15 | ğŸŸ  MEDIUM |
| **Train vs Test Gap** | Overfitting indicator | Small | ğŸŸ  MEDIUM |
| **Training Time** | Speed of model | < 10 sec | ğŸŸ¡ LOW |

## Step 1: Compare in MLflow UI

### Open MLflow
```
http://127.0.0.1:5000
â†’ Experiments
â†’ Archive-Forecast-ML-POC
```

### Select Both Runs
```
â˜‘ï¸ Run 1 (model-1)
â˜‘ï¸ Run 2 (model-2)
Click: "Compare" button
```

---

## Step 2: Analyze Key Metrics

### Test RÂ² (Most Important)
```
Higher is better: 0.75+

Model A: test_r2 = 0.8543 âœ… BETTER
Model B: test_r2 = 0.7541

â†’ Model A explains more variance in test data
```

### Test MAE (Most Important)
```
Lower is better: < 10 GB

Model A: test_mae = 5.2 GB âœ… BETTER
Model B: test_mae = 6.1 GB

â†’ Model A makes smaller prediction errors
```

### Test RMSE (Secondary)
```
Lower is better: < 15 GB

Model A: test_rmse = 7.8 GB âœ… BETTER
Model B: test_rmse = 8.9 GB

â†’ Model A handles large errors better
```

---

## Step 3: Check for Overfitting

### Compare Train vs Test Metrics

**Healthy Model (Low Overfitting):**
```
Train MAE:  1.5 GB
Test MAE:   6.0 GB
Gap: 4.5 GB  âœ… Reasonable (test is ~4x train)

Train RÂ²: 0.95
Test RÂ²:  0.75
Gap: 0.20  âœ… Acceptable
```

**Overfit Model (Red Flag):**
```
Train MAE:  0.5 GB
Test MAE:  15.0 GB
Gap: 14.5 GB  âš ï¸ TOO LARGE (model memorized training)

Train RÂ²: 0.98
Test RÂ²:  0.50
Gap: 0.48  âš ï¸ CONCERNING
```

---

## Complete Model Comparison Checklist

### âœ… Test Metrics (Most Important)
```
â–¡ Test RÂ² > 0.75         â†’ Explains variance well
â–¡ Test MAE < 10 GB       â†’ Reasonable error
â–¡ Test RMSE < 15 GB      â†’ Handles outliers OK
```

### âœ… Overfitting Check (Important)
```
â–¡ Train RÂ² - Test RÂ² < 0.25      â†’ Not overfitting
â–¡ Train MAE / Test MAE < 5x      â†’ Reasonable gap
â–¡ Validation loss plateaued       â†’ Not improving
```

### âœ… Consistency (Nice to Have)
```
â–¡ Train MAE low                   â†’ Learns training data
â–¡ Test metrics stable             â†’ Consistent generalization
â–¡ Training time reasonable        â†’ < 30 seconds
```

---

## Real Example: Which Model to Choose?

### Scenario: Comparing 3 Models

```
MODEL A (seed=42):
  test_r2:   0.8543 âœ…
  test_mae:  5.2 GB âœ…
  test_rmse: 7.8 GB âœ…
  train_r2:  0.9538
  train_mae: 1.5 GB
  overfitting: LOW âœ…

MODEL B (seed=123):
  test_r2:   0.7541
  test_mae:  6.1 GB
  test_rmse: 8.9 GB
  train_r2:  0.9521
  train_mae: 1.5 GB
  overfitting: LOW âœ…

MODEL C (seed=999):
  test_r2:   0.6234 âš ï¸
  test_mae:  12.4 GB âš ï¸
  test_rmse: 18.2 GB âš ï¸
  train_r2:  0.9612
  train_mae: 1.2 GB
  overfitting: MEDIUM âš ï¸
```

**ğŸ† BEST: Model A**
- Highest test RÂ² (0.8543)
- Lowest test MAE (5.2 GB)
- Good overfitting balance
- Consistently best across all metrics

---

## Step-by-Step Guide in MLflow

### 1. Open Comparison View
```
MLflow UI
â†’ Experiments â†’ Archive-Forecast-ML-POC
â†’ Select Run 1 â˜‘ï¸
â†’ Select Run 2 â˜‘ï¸
â†’ Click "Compare" button
```

### 2. View Metrics Table
```
Metrics Tab shows:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric          â”‚ Run 1  â”‚ Run 2  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test_r2         â”‚ 0.854  â”‚ 0.754  â”‚
â”‚ test_mae        â”‚ 5.2    â”‚ 6.1    â”‚
â”‚ test_rmse       â”‚ 7.8    â”‚ 8.9    â”‚
â”‚ train_r2        â”‚ 0.954  â”‚ 0.952  â”‚
â”‚ train_mae       â”‚ 1.5    â”‚ 1.5    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Analyze Differences
```
âœ… Run 1 Better In: test_r2, test_mae, test_rmse
âœ… Run 2 Better In: (none, Run 1 wins overall)
```

### 4. Make Decision
```
ğŸ† Choose Run 1 (best test metrics)
```

---

## Decision Framework: Flowchart

```
START: Comparing 2+ Models
    â†“
1. Look at Test RÂ²
    â”œâ”€ RÂ² > 0.80  â†’ Continue âœ…
    â””â”€ RÂ² < 0.70  â†’ Eliminate model âŒ
    â†“
2. Look at Test MAE
    â”œâ”€ MAE < 8 GB   â†’ Continue âœ…
    â””â”€ MAE > 12 GB  â†’ Eliminate model âŒ
    â†“
3. Check Overfitting
    â”œâ”€ Train-Test gap < 0.25 â†’ Continue âœ…
    â””â”€ Train-Test gap > 0.35 â†’ Eliminate model âŒ
    â†“
4. If Still Tied
    â”œâ”€ Choose: Lower training time
    â”œâ”€ Choose: More consistent metrics
    â””â”€ Choose: Simpler parameters
    â†“
DECISION: Best Model Selected âœ…
```

---

## Scoring System: Quantify Which is Best

### Assign Points (1-5)

```python
# For each metric, higher is better

MODEL A Scoring:
  test_r2 (0.8543):    5 points (excellent)
  test_mae (5.2 GB):   5 points (excellent)
  test_rmse (7.8 GB):  5 points (good)
  overfitting:         5 points (minimal)
  training_time:       4 points (reasonable)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL: 24/25 points  â­â­â­â­â­

MODEL B Scoring:
  test_r2 (0.7541):    3 points (acceptable)
  test_mae (6.1 GB):   4 points (good)
  test_rmse (8.9 GB):  4 points (good)
  overfitting:         4 points (low)
  training_time:       4 points (reasonable)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL: 19/25 points  â­â­â­â­

ğŸ† WINNER: Model A (24 > 19)
```

---

## When Models are VERY Similar

### Example: Scores Differ by < 2 Points
```
Model A: 22/25 points
Model B: 21/25 points

â†’ Virtually equivalent, choose by:
  1. Training time (faster wins)
  2. Parameter simplicity (fewer hyperparams wins)
  3. Feature stability (consistent importance wins)
  4. Visual inspection (artifacts/plots)
```

---

## What Each Metric Really Means

### RÂ² (Coefficient of Determination)
```
RÂ² = 0.85 means:
  "Model explains 85% of variance in test data"
  
Interpretation:
  RÂ² > 0.80  â†’ Excellent fit
  RÂ² 0.70-80 â†’ Good fit
  RÂ² 0.60-70 â†’ Acceptable fit
  RÂ² < 0.60  â†’ Poor fit âŒ

Example:
  Test RÂ² = 0.8543  âœ… This model is very good
  Test RÂ² = 0.5234  âŒ This model is not reliable
```

### MAE (Mean Absolute Error)
```
MAE = 5.2 GB means:
  "On average, predictions are off by Â±5.2 GB"
  
Interpretation:
  If actual = 50 GB, prediction â‰ˆ 44-56 GB
  
Lower is better:
  MAE < 5 GB   â†’ Excellent accuracy
  MAE 5-10 GB  â†’ Good accuracy
  MAE 10-20 GB â†’ Acceptable
  MAE > 20 GB  â†’ Poor âŒ
```

### RMSE (Root Mean Squared Error)
```
RMSE = 7.8 GB means:
  "Error considering larger mistakes more heavily"
  
When to use:
  â€¢ When large errors are worse
  â€¢ When you want to penalize outliers
  
Comparison:
  RMSE > MAE always (penalizes big errors)
  If RMSE â‰ˆ MAE â†’ Errors are consistent
  If RMSE >> MAE â†’ Some very large errors
```

---

## Common Pitfalls to Avoid

### âŒ Mistake 1: Only Looking at Training Metrics
```
WRONG: "Model A has train_r2=0.98, it's the best!"
RIGHT: Compare TEST metrics (generalization)
```

### âŒ Mistake 2: Ignoring Overfitting
```
WRONG: train_r2=0.99, test_r2=0.50
  â†“ Model memorized training data, won't generalize

RIGHT: train_r2=0.95, test_r2=0.80
  â†“ Healthy generalization
```

### âŒ Mistake 3: Focusing on Single Metric
```
WRONG: "Model A has higher MAE, so it's worse"
RIGHT: Consider all metrics: RÂ², MAE, RMSE, overfitting

Example:
  Model A: test_r2=0.85, test_mae=6.5 âœ…
  Model B: test_r2=0.70, test_mae=5.0 âŒ (overall worse)
```

### âŒ Mistake 4: Not Checking Statistical Significance
```
WRONG: "Model A=0.754, Model B=0.753, A is better"
RIGHT: Difference of 0.001 is noise, run multiple times

Solution: Train 3-5 times, compare averages
```

---

## Advanced: Statistical Comparison

### Run Multiple Times, Take Average

```bash
# Train with same data, different seeds
python src/ml/train_with_mlflow.py --experiment-name "model-a-v1"
python src/ml/train_with_mlflow.py --experiment-name "model-a-v2"
python src/ml/train_with_mlflow.py --experiment-name "model-a-v3"
```

### Compare Averages
```
Model A (3 runs):
  avg test_rÂ²:   0.8543, 0.8521, 0.8535  â†’ avg = 0.8533 âœ…
  std dev:       0.0010 (very stable)

Model B (3 runs):
  avg test_rÂ²:   0.7541, 0.7523, 0.7685  â†’ avg = 0.7583
  std dev:       0.0082 (more variable)

â†’ Model A is more stable AND higher performing
```

---

## Final Decision: Best Model Checklist

### Before Selecting Best Model, Verify:

```
â–¡ Test RÂ² > 0.75
â–¡ Test MAE < 10 GB
â–¡ Train-Test gap < 0.25
â–¡ No overfitting signs
â–¡ Model trained on representative data
â–¡ Multiple runs show consistency
â–¡ Error distribution looks reasonable
â–¡ Business requirements met
```

### If ALL âœ…:
```
â†’ Model is READY FOR PRODUCTION
```

### If ANY âŒ:
```
â†’ Retrain with different parameters
â†’ Get more data
â†’ Engineer better features
```

---

## Your Next Steps

### 1. Compare Your 2 Models Right Now
```
MLflow UI â†’ Compare
Identify: Which has higher test_r2?
```

### 2. Calculate the Scoring
```
Use the scoring table above
Count total points
Winner: Higher score
```

### 3. Check Overfitting
```
train_r2 - test_r2 = ?
  < 0.20  âœ… Good
  0.20-30 âš ï¸ Watch
  > 0.30  âŒ Retrain
```

### 4. Make Final Decision
```
Select best model
Note: Run ID
Location: models/model.joblib
```

### 5. Document Decision
```
Why this model won:
  â€¢ Higher test RÂ²
  â€¢ Lower test MAE
  â€¢ Minimal overfitting
  â€¢ Consistent metrics
```

---

## Example Output Format

```
COMPARISON RESULTS
==================

Run 1 (seed=42):
  Test RÂ²:   0.8543
  Test MAE:  5.2 GB
  Overfitting: 0.13 (good)
  Score: â­â­â­â­â­ (24/25)

Run 2 (seed=123):
  Test RÂ²:   0.7541
  Test MAE:  6.1 GB
  Overfitting: 0.14 (good)
  Score: â­â­â­â­ (19/25)

WINNER: Run 1 âœ…
Reason: Higher test RÂ², lower test MAE
Next Step: Deploy models/model.joblib
```

---

**Key Takeaway: Compare test metrics, check overfitting, then decide based on overall performance!** ğŸ¯
