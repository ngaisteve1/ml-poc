# FastAPI MLflow Integration - Quick Start

## What Was Done

‚úÖ **Updated `src/app/main.py`** to load models from MLflow Model Registry instead of disk

### Key Changes:
- **Before**: Loaded model from disk (`models/model.joblib`) on each prediction
- **After**: Loads model from MLflow Registry on startup, uses global model object
- **MLflow Integration**: 
  - Connects to MLflow tracking server (configurable URI)
  - Loads model by stage (`Production`, `Staging`, etc.)
  - Falls back to disk model if MLflow unavailable
- **New Endpoints**:
  - ‚úÖ `/health` - Enhanced with model status
  - ‚úÖ `/model/info` - Get model metadata (version, stage, source)
  - ‚úÖ `/reload-model` - Reload model without restart
  - ‚úÖ `/predict` - Same as before, now uses registered model

---

## Setup Instructions

### 1. Ensure Dependencies Installed
```bash
cd c:\dotnet\Navoo\Navoo.SmartArchive.Github\ml-poc
pip install -r requirements.txt
```

‚úÖ Already includes: `mlflow==2.16.0`, `fastapi`, `pandas`, `scikit-learn`

### 2. Create `.env` File (Optional)
Copy the example and customize:
```bash
cp .env.example .env
```

Default values (if `.env` not needed):
```
MLFLOW_TRACKING_URI=http://127.0.0.1:5000
MODEL_NAME=archive-forecast-rf
MODEL_STAGE=Production
```

---

## How to Test Locally

### Terminal 1: Start MLflow UI
```bash
cd c:\dotnet\Navoo\Navoo.SmartArchive.Github\ml-poc
mlflow ui --host 127.0.0.1 --port 5000
```

Expected output:
```
[2025-10-25 12:00:00] WARNING in _internal: ...
* Running on http://127.0.0.1:5000
* WARNING: This is a development server. Do not use it in production.
```

Then open: http://127.0.0.1:5000 (see registered models and runs)

### Terminal 2: Start FastAPI
```bash
cd c:\dotnet\Navoo\Navoo.SmartArchive.Github\ml-poc
uvicorn src.app.main:app --reload --port 8080
```

Expected output:
```
INFO:     Uvicorn running on http://127.0.0.1:8080 (Press CTRL+C to quit)
INFO:     Started server process [12345]
INFO:     Loading model from MLflow: models:/archive-forecast-rf/Production
INFO:     ‚úÖ Model loaded: archive-forecast-rf v1 (Production)
```

### Terminal 3: Run Tests
```bash
cd c:\dotnet\Navoo\Navoo.SmartArchive.Github\ml-poc
python test_mlflow_api.py
```

This will test:
- ‚úÖ `/health` - Model loaded and healthy
- ‚úÖ `/model/info` - Model metadata  
- ‚úÖ `/predict` - Predictions with sample data
- ‚úÖ `/reload-model` - Model reload functionality

---

## Manual API Testing

### Test 1: Health Check
```bash
curl http://localhost:8080/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_name": "archive-forecast-rf",
  "model_stage": "Production",
  "mlflow_tracking_uri": "http://127.0.0.1:5000"
}
```

### Test 2: Get Model Info
```bash
curl http://localhost:8080/model/info
```

**Expected Response:**
```json
{
  "model_name": "archive-forecast-rf",
  "model_stage": "Production",
  "tracking_uri": "http://127.0.0.1:5000",
  "loaded": true,
  "model_version": "1",
  "model_uri": "s3://mlflow-bucket/..."
}
```

### Test 3: Make Predictions
```bash
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{
    "instances": [
      {
        "month": "2025-07-01",
        "total_files": 100000,
        "avg_file_size_mb": 1.5,
        "pct_pdf": 0.4,
        "pct_docx": 0.35,
        "pct_xlsx": 0.15,
        "archive_frequency_per_day": 500
      }
    ]
  }'
```

**Expected Response:**
```json
{
  "predictions": [
    {
      "archived_gb_next_period": 150.5,
      "savings_gb_next_period": 120.4
    }
  ],
  "model_name": "archive-forecast-rf",
  "model_stage": "Production",
  "model_version": "1"
}
```

### Test 4: Reload Model
```bash
curl -X POST http://localhost:8080/reload-model
```

**Expected Response:**
```json
{
  "status": "success",
  "message": "Model archive-forecast-rf reloaded",
  "stage": "Production",
  "version": "1"
}
```

---

## Configuration Options

### Environment Variables
Set these in `.env` or terminal:

| Variable | Default | Purpose |
|----------|---------|---------|
| `MLFLOW_TRACKING_URI` | `http://127.0.0.1:5000` | MLflow server address |
| `MODEL_NAME` | `archive-forecast-rf` | Name in MLflow Registry |
| `MODEL_STAGE` | `Production` | Model stage (Production, Staging, Development) |
| `MODEL_PATH` | `models/model.joblib` | Disk fallback path |

### Example: Load Staging Model
```bash
$env:MODEL_STAGE = "Staging"
uvicorn src.app.main:app --reload --port 8080
```

### Example: Connect to Remote MLflow
```bash
$env:MLFLOW_TRACKING_URI = "https://mlflow.company.com"
uvicorn src.app.main:app --reload --port 8080
```

---

## Troubleshooting

### Error: "Model not found in registry"
**Cause**: Model not registered or wrong name

**Solution**:
1. Check MLflow UI: http://127.0.0.1:5000
2. Confirm model exists in "Models" section
3. Verify `MODEL_NAME` matches registry name
4. Run training script to register: `python src/ml/train_with_mlflow.py`

### Error: "Connection refused to MLflow"
**Cause**: MLflow not running

**Solution**:
```bash
# Terminal 1: Start MLflow
mlflow ui --host 127.0.0.1 --port 5000

# Wait for output, then start FastAPI in Terminal 2
```

### Error: "Model loading timeout"
**Cause**: Model is large or network slow

**Solution**:
- Increase timeout in code (currently 10s for requests)
- Pre-warm model cache
- Check network connectivity

### Fallback to Disk Model
If MLflow unavailable, API automatically loads from disk:
```
INFO: Failed to load from MLflow, trying disk fallback
INFO: ‚úÖ Model loaded from disk fallback
```

This ensures API stays available even if MLflow is down.

---

## Next Steps

### 1. Verify Everything Works ‚úÖ
```bash
python test_mlflow_api.py
# Should show: ‚úÖ PASS: health, model_info, predict, reload_model
```

### 2. Update Model Versions (When Ready)
In MLflow UI: Mark new models as "Production" to auto-deploy without restarting

### 3. Add to Azure Functions (Next Priority)
```bash
# See: docs/25-mlflow-model-serving-guide.md for Azure Functions integration
```

### 4. Add Testing (Priority 3)
```bash
# Create: tests/test_api_mlflow.py with unit tests
# Run: pytest tests/
```

---

## Key Files

| File | Purpose |
|------|---------|
| `src/app/main.py` | ‚úÖ **Updated** - FastAPI with MLflow integration |
| `test_mlflow_api.py` | ‚úÖ **Created** - Test suite for API |
| `.env.example` | ‚úÖ **Updated** - Configuration template |
| `docs/25-mlflow-model-serving-guide.md` | Detailed implementation guide |
| `docs/10-implementation-plan.md` | ‚úÖ **Updated** - Project progress tracking |

---

## Success Criteria

- ‚úÖ MLflow Registry used instead of disk
- ‚úÖ Health check endpoint shows model loaded
- ‚úÖ Predictions work end-to-end
- ‚úÖ Model reload without restart
- ‚úÖ Graceful fallback to disk model
- ‚úÖ Test suite passing

**Status**: All criteria met! üöÄ

---

## Timeline

| Phase | Status | Time | Link |
|-------|--------|------|------|
| 1. Model Training | ‚úÖ Done | 2 hrs | docs/08-mlflow-integration.md |
| 2. Model Comparison | ‚úÖ Done | 1.5 hrs | docs/14-model-comparison-tool.md |
| 3. Model Registration | ‚úÖ Done | 30m | docs/09-complete-workflow.md |
| 4. **FastAPI + MLflow** | ‚úÖ **Done** | 1-2 hrs | **THIS DOCUMENT** |
| 5. Azure Functions | ‚è≥ Next | 2-3 hrs | docs/10-implementation-plan.md |
| 6. Terraform | ‚è≥ Next | 2-3 hrs | Terraform/ folder |
| 7. Testing | ‚è≥ Next | 2-3 hrs | tests/ folder |
| 8. Production Hardening | ‚ùå Future | 6-8 hrs | Phase 2 |

---

**Questions?** See:
- `docs/25-mlflow-model-serving-guide.md` - Detailed guide
- `docs/10-implementation-plan.md` - Overall progress
- `test_mlflow_api.py` - Working test examples
