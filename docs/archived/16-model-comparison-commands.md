â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   MODEL COMPARISON TOOL - QUICK COMMANDS                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ MOST COMMON USE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
After training your models in MLflow, compare them automatically:

    python src/ml/compare_models.py

This will:
  âœ… Fetch all your trained models
  âœ… Calculate quality scores (1-23 points total)
  âœ… Rank them automatically
  âœ… Show best model with analysis
  âœ… Provide recommendations


ğŸ“Š WHAT YOU'LL SEE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MODEL COMPARISON RESULTS
================================================================================

Ranking:
Rank  Run ID      Test RÂ²    Test MAE     Test RMSE    Overfit    Score
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1     abc123      0.8543     5.21 GB      7.89 GB      0.1234     23/23 â­â­â­â­â­
2     def456      0.7541     6.06 GB      8.91 GB      0.2000     19/23 â­â­â­â­

ğŸ† BEST MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Run ID: abc123e5f6g7h8i9j0k1l2m3n4o5p6
Test RÂ²: 0.8543 âœ…
Test MAE: 5.21 GB âœ…
Score: 23/23 â­â­â­â­â­


ğŸ”§ OTHER COMMANDS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# View only top 3 models
python src/ml/compare_models.py --top 3

# Use different experiment name
python src/ml/compare_models.py --experiment-name "my-experiment"

# Connect to different MLflow server
python src/ml/compare_models.py --mlflow-uri "http://other-server:5000"

# Combine options
python src/ml/compare_models.py --top 5 --experiment-name "production" --mlflow-uri "http://prod:5000"


ğŸ“ˆ WORKFLOW EXAMPLE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 1: Train first model
$ python src/ml/train_with_mlflow.py
âœ… Logged to MLflow (Run: abc123)

Step 2: Train second model with different parameters
$ python src/ml/train_with_mlflow.py
âœ… Logged to MLflow (Run: def456)

Step 3: Compare models
$ python src/ml/compare_models.py
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODEL COMPARISON RESULTS                            â”‚
â”‚                                                     â”‚
â”‚ 1. abc123 - 23/23 â­â­â­â­â­ BEST MODEL (USE THIS)  â”‚
â”‚ 2. def456 - 19/23 â­â­â­â­                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 4: Model at models/model.joblib is ready for deployment


â­ KEY METRICS EXPLAINED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Test RÂ² (Coefficient of Determination)
  â”œâ”€ What: How much variance does model explain?
  â”œâ”€ Range: 0.0 to 1.0
  â”œâ”€ Better: HIGHER is better
  â”œâ”€ Points: >0.80 = 5 pts â­â­â­â­â­, >0.75 = 4 pts, >0.70 = 3 pts
  â””â”€ Target: > 0.75 âœ…

Test MAE (Mean Absolute Error)
  â”œâ”€ What: Average prediction error in GB
  â”œâ”€ Example: 5.21 GB means predictions off by ~5 GB on average
  â”œâ”€ Better: LOWER is better
  â”œâ”€ Points: <5 GB = 5 pts â­â­â­â­â­, <8 GB = 4 pts, <10 GB = 3 pts
  â””â”€ Target: < 10 GB âœ…

Test RMSE (Root Mean Squared Error)
  â”œâ”€ What: Like MAE but penalizes large errors more
  â”œâ”€ Better: LOWER is better
  â”œâ”€ Points: <8 GB = 5 pts, <10 GB = 4 pts, <12 GB = 3 pts
  â””â”€ Target: < 12 GB âœ…

Overfitting (Train RÂ² - Test RÂ²)
  â”œâ”€ What: Is model memorizing training data?
  â”œâ”€ Good: SMALL gap (<0.20) = generalizes well
  â”œâ”€ Bad: BIG gap (>0.30) = overfits to training data
  â”œâ”€ Points: <0.10 gap = 5 pts â­â­â­â­â­, <0.20 = 3 pts
  â””â”€ Example: gap of 0.1234 = Good âœ…


âœ… SCORING SYSTEM (Total: 0-23 Points):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Metric             | Max Points | Threshold for Full Points
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Test RÂ²            |     5      | > 0.80
  Test MAE           |     5      | < 5 GB
  Test RMSE          |     5      | < 8 GB
  Overfitting        |     5      | Gap < 0.10
  Speed (optional)   |     3      | < 5 seconds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL              |    23      |

Score Interpretation:
  20-23: â­â­â­â­â­ Excellent - Production ready
  17-19: â­â­â­â­ Good - Production ready with monitoring
  14-16: â­â­â­ Acceptable - Needs improvement
  11-13: â­â­ Poor - Needs significant work
   0-10: â­ Very Poor - Not recommended


ğŸ“ EXAMPLE: COMPARING 3 MODELS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

You train 3 models with different hyperparameters:

Model A (n_estimators=100, max_depth=10)
  Test RÂ²: 0.8543 (5 pts)
  Test MAE: 5.21 GB (5 pts)
  Test RMSE: 7.89 GB (5 pts)
  Overfit: 0.1234 (3 pts)
  Speed: 2.3 sec (3 pts)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL: 21/23 pts â­â­â­â­â­

Model B (n_estimators=50, max_depth=5)
  Test RÂ²: 0.7541 (4 pts)
  Test MAE: 6.06 GB (4 pts)
  Test RMSE: 8.91 GB (4 pts)
  Overfit: 0.2000 (2 pts)
  Speed: 1.2 sec (3 pts)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL: 17/23 pts â­â­â­â­

Model C (n_estimators=200, max_depth=15)
  Test RÂ²: 0.7200 (3 pts)
  Test MAE: 8.50 GB (3 pts)
  Test RMSE: 12.30 GB (3 pts)
  Overfit: 0.3500 (1 pt)
  Speed: 5.1 sec (2 pts)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL: 12/23 pts â­â­

ğŸ† WINNER: Model A (21 pts) â† USE THIS FOR PRODUCTION


â“ COMMON QUESTIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q: "My best model scored 18/23. Is it good enough?"
A: Yes! Score >15 is production-ready. This scores 18 = Good.
   Test RÂ²=0.77 and Test MAE=8 GB are acceptable thresholds.

Q: "Two models have same score. Which to pick?"
A: Tiebreaker order:
   1. Higher Test RÂ² (explains more variance)
   2. Lower Test MAE (more accurate)
   3. Lower overfitting (better generalization)
   4. Faster training (lower operational cost)

Q: "What if all models score poorly (<12)?"
A: Your model needs improvement:
   1. Get more data: python src/ml/generate_mock_data.py --rows 48
   2. Try new features: edit train_with_mlflow.py
   3. Different algorithm: use different sklearn models
   4. Hyperparameter tuning: adjust max_depth, n_estimators, etc.

Q: "Can I modify the scoring thresholds?"
A: Yes! Edit compare_models.py calculate_score() function:
   
   if r2 > 0.75:  # Change 0.75 to your threshold
       breakdown['test_r2_score'] = 4

Q: "How often should I retrain?"
A: After major data changes or monthly, whichever comes first.
   1. Generate new data: python src/ml/generate_mock_data.py
   2. Train new model: python src/ml/train_with_mlflow.py
   3. Compare models: python src/ml/compare_models.py
   4. Deploy best one if score > current production model

Q: "Can I use this with production data?"
A: Yes! Just ensure data is in same format as training_data.csv:
   - Columns: period, files_archived, volume_gb, ... (13 total)
   - Format: CSV with headers
   - Location: data/training_data.csv


ğŸš€ NEXT STEPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Generate mock data:
   python src/ml/generate_mock_data.py --rows 24

2. Train models (run multiple times):
   python src/ml/train_with_mlflow.py
   python src/ml/train_with_mlflow.py
   python src/ml/train_with_mlflow.py

3. Compare them:
   python src/ml/compare_models.py

4. Pick winner and deploy to production

5. Monitor performance, retrain monthly


ğŸ“– RELATED DOCUMENTATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For more details, see:
  â€¢ docs/13-model-selection.md - Manual decision framework
  â€¢ docs/14-model-comparison-tool.md - Full tool documentation
  â€¢ docs/08-mlflow-integration.md - MLflow setup guide
  â€¢ docs/09-complete-workflow.md - End-to-end workflow

For more help:
  â€¢ Check MLflow UI: http://127.0.0.1:5000
  â€¢ Review model artifacts and metrics
  â€¢ Compare runs side-by-side


âœ¨ TIPS & TRICKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ’¡ Keep MLflow UI open while comparing:
   http://127.0.0.1:5000 â†’ Models tab â†’ Compare runs

ğŸ’¡ Export comparison results:
   python src/ml/compare_models.py > model_comparison.txt

ğŸ’¡ Track which model is "best":
   â€¢ Note the Run ID
   â€¢ Add it to your deployment README
   â€¢ Reference it in production

ğŸ’¡ Version control your models:
   â€¢ Save best model path
   â€¢ Document score and metrics
   â€¢ Keep old versions for rollback


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ REMEMBER: Your best model is now at models/model.joblib
              Ready for deployment and predictions!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For questions or issues: See docs/06-troubleshooting.md or inspect MLflow logs

Last Updated: 2025-01-01
Tool Version: 1.0
Status: âœ… Production Ready
